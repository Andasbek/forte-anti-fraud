–ù–∏–∂–µ –æ—Ç–¥–µ–ª—å–Ω–∞—è, –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ **ML-—á–∞—Å—Ç–∏** –ø—Ä–æ–µ–∫—Ç–∞. –ï—ë –º–æ–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å –≤ README –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª `ml/README.md`.

---

# üìä ML-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: –º–æ–¥–µ–ª—å –∞–Ω—Ç–∏—Ñ—Ä–æ–¥–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ –±–∞–Ω–∫–∏–Ω–≥–∞

## 1. –¶–µ–ª—å –º–æ–¥–µ–ª–∏

–ú–æ–¥–µ–ª—å —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É **–±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**:

> –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è **–º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–æ–π** (`is_fraud = 1`) –∏–ª–∏ **—á–∏—Å—Ç–æ–π** (`is_fraud = 0`) –Ω–∞ –æ—Å–Ω–æ–≤–µ:
>
> * –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–∞–º–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Å—É–º–º–∞, –∫–ª–∏–µ–Ω—Ç),
> * –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –ª–æ–≥–∞–º –º–æ–±–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.

–ú–µ—Ç—Ä–∏–∫–∞ —Ñ–æ–∫—É—Å–∞:

* **ROC-AUC** –∏ **PR-AUC** (—Å —É—á—ë—Ç–æ–º —Å–∏–ª—å–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤).

–¢–µ–∫—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (random split):

* **ROC-AUC ‚âà 0.8798**
* **PR-AUC ‚âà 0.4767**

---

## 2. –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö

–û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä—ã —Ö–∞–∫–∞—Ç–æ–Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∏ **–¥–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞**:

1. `data.csv` ‚Äî **—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏**

   –ü—Ä–∏–º–µ—Ä —á—Ç–µ–Ω–∏—è:

   ```python
   import pandas as pd

   data = pd.read_csv("data.csv", encoding="cp1251", sep=";")
   ```

   –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤ –∏—Å—Ö–æ–¥–Ω–æ–º –≤–∏–¥–µ):

   * `–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞`
   * `–î–∞—Ç–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏`
   * `–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏`
   * `–°—É–º–º–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞`
   * `–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏`
   * `–ó–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞—Ç–µ–ª—è ...`
   * `–†–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏(...)` ‚Üí —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è `is_fraud`

2. `data2.csv` ‚Äî **–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** –ø–æ –ª–æ–≥–∞–º –º–æ–±–∏–ª—å–Ω–æ–≥–æ –±–∞–Ω–∫–∞

   –ü—Ä–∏–º–µ—Ä —á—Ç–µ–Ω–∏—è:

   ```python
   data2 = pd.read_csv("data2.csv", encoding="cp1251", sep=";")
   ```

   –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:

   * `–î–∞—Ç–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏`
   * `–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞`
   * –î–∞–ª–µ–µ –Ω–∞–±–æ—Ä feature-–∫–æ–ª–æ–Ω–æ–∫:

     * –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –û–° –∑–∞ 30 –¥–Ω–µ–π,
     * –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∑–∞ 30 –¥–Ω–µ–π,
     * –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–≥–∏–Ω-—Å–µ—Å—Å–∏–π –∑–∞ 7/30 –¥–Ω–µ–π,
     * —Å—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –ª–æ–≥–∏–Ω–æ–≤ –≤ –¥–µ–Ω—å,
     * –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –ª–æ–≥–∏–Ω–æ–≤,
     * –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ (mean, std, var, EWM),
     * –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å ‚Äú–≤–∑—Ä—ã–≤–Ω–æ—Å—Ç–∏‚Äù –ª–æ–≥–∏–Ω–æ–≤,
     * Fano-factor,
     * Z-—Å–∫–æ—Ä –Ω–µ–¥–∞–≤–Ω–∏—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ vs 30-–¥–Ω–µ–≤–Ω—ã—Ö –∏ —Ç.–¥.

---

## 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

### 3.1. –ß—Ç–µ–Ω–∏–µ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–æ–≤

```python
import pandas as pd

data = pd.read_csv("data.csv", encoding="cp1251", sep=";")
data2 = pd.read_csv("data2.csv", encoding="cp1251", sep=";")
```

–ö–æ–ª–æ–Ω–∫–∞ –¥–∞—Ç—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–º–µ–ª–∞ –≤–∏–¥:

```text
cst_dim_id;transdate;transdatetime;amount;...
2937833270;'2025-01-05 00:00:00.000';'2025-01-05 16:32:02.000';...
```

–ü–æ—ç—Ç–æ–º—É:

* —Å–Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª–∏ –ø–æ–ª—è –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç,
* –∑–∞—Ç–µ–º –ø—Ä–∏–≤–µ–ª–∏ —Å—Ç—Ä–æ–∫–∏ –∫ `datetime`.

–ü—Ä–∏–º–µ—Ä:

```python
# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Ä—É—Å—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ
rename_map_trans = {
    "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞": "client_id",
    "–î–∞—Ç–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏": "transdate",
    "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏": "transdatetime",
    "–°—É–º–º–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞": "amount",
    "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏": "transaction_id",
    "–ó–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞—Ç–µ–ª—è/destination —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏": "destination_id",
    "–†–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏(–ø–µ—Ä–µ–≤–æ–¥—ã), –≥–¥–µ 1 - –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è , 0 - —á–∏—Å—Ç–∞—è": "is_fraud",
}

data = data.rename(columns=rename_map_trans)

# –î–∞—Ç—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
data["transdate"] = pd.to_datetime(
    data["transdate"].astype(str).str.strip("'"),
    format="%Y-%m-%d %H:%M:%S.%f",
    errors="coerce",
)
data["transdatetime"] = pd.to_datetime(
    data["transdatetime"].astype(str).str.strip("'"),
    format="%Y-%m-%d %H:%M:%S.%f",
    errors="coerce",
)

# –î–æ–ø. —á–∞—Å–æ–≤–∞—è –¥–∞—Ç–∞ –¥–ª—è join‚Äô–∞
data["trans_date"] = data["transdate"].dt.date
```

–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è `data2`:

```python
rename_map_beh = {
    "–î–∞—Ç–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏": "trans_date",
    "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞": "client_id",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –û–° (os_ver) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π –¥–æ transdate ‚Äî —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –û–°/–≤–µ—Ä—Å–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –∫–ª–∏–µ–Ω—Ç": "os_ver_cnt_30d",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–µ–ª–µ—Ñ–æ–Ω–∞ (phone_model) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–æ –∫–ª–∏–µ–Ω—Ç ‚Äú–º–µ–Ω—è–ª —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ‚Äù –ø–æ –ª–æ–≥–∞–º": "phone_model_cnt_30d",
    "–ú–æ–¥–µ–ª—å —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Å–∞–º–æ–π –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏ (–ø–æ –≤—Ä–µ–º–µ–Ω–∏) –ø–µ—Ä–µ–¥ transdate": "phone_model_last",
    "–í–µ—Ä—Å–∏—è –û–° –∏–∑ —Å–∞–º–æ–π –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏ –ø–µ—Ä–µ–¥ transdate": "os_version_last",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–≥–∏–Ω-—Å–µ—Å—Å–∏–π (–º–∏–Ω—É—Ç–Ω—ã—Ö —Ç–∞–π–º-—Å–ª–æ—Ç–æ–≤) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π –¥–æ transdate": "login_sessions_7d",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–≥–∏–Ω-—Å–µ—Å—Å–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π –¥–æ transdate": "login_sessions_30d",
    "–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –ª–æ–≥–∏–Ω–æ–≤ –≤ –¥–µ–Ω—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π: logins_last_7_days / 7": "logins_per_day_7d",
    "–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –ª–æ–≥–∏–Ω–æ–≤ –≤ –¥–µ–Ω—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π: logins_last_30_days / 30": "logins_per_day_30d",
    "–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 7 –¥–Ω–µ–π –∫ —Å—Ä–µ–¥–Ω–µ–π —á–∞—Å—Ç–æ—Ç–µ –∑–∞ 30 –¥–Ω–µ–π:\n(freq7d?freq30d)/freq30d(freq_{7d} - freq_{30d}) / freq_{30d}(freq7d?freq30d)/freq30d ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —Å—Ç–∞–ª –∫–ª–∏–µ–Ω—Ç –∑–∞—Ö–æ–¥–∏—Ç—å —á–∞—â–µ –∏–ª–∏ —Ä–µ–∂–µ –Ω–µ–¥–∞–≤–Ω–æ": "login_freq_change_7d_vs_30d",
    "–î–æ–ª—è –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 7 –¥–Ω–µ–π –æ—Ç –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 30 –¥–Ω–µ–π": "logins_7d_share_of_30d",
    "–°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö) –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Å–µ—Å—Å–∏—è–º–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π": "avg_session_interval_30d",
    "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 30 –¥–Ω–µ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö), –∏–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–±—Ä–æ—Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤": "std_session_interval_30d",
    "–î–∏—Å–ø–µ—Ä—Å–∏—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 30 –¥–Ω–µ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö?), –µ—â—ë –æ–¥–Ω–∞ –º–µ—Ä–∞ —Ä–∞–∑–±—Ä–æ—Å–∞": "var_session_interval_30d",
    "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 7 –¥–Ω–µ–π, –≥–¥–µ –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ —Å–µ—Å—Å–∏–∏ –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è 0.3)": "ewm_session_interval_7d",
    "–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å ‚Äú–≤–∑—Ä—ã–≤–Ω–æ—Å—Ç–∏‚Äù –ª–æ–≥–∏–Ω–æ–≤: (std?mean)/(std+mean)(std - mean)/(std + mean)(std?mean)/(std+mean) –¥–ª—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤": "burstiness_sessions",
    "Fano-factor –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤: variance / mean": "fano_factor_sessions",
    "Z-—Å–∫–æ—Ä —Å—Ä–µ–¥–Ω–µ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–∞ 30 –¥–Ω–µ–π: –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Ç–∏–ø–∏—á–Ω—ã—Ö, –≤ –µ–¥–∏–Ω–∏—Ü–∞—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è": "zscore_interval_7d_vs_30d",
}

data2 = data2.rename(columns=rename_map_beh)

data2["trans_date"] = pd.to_datetime(
    data2["trans_date"].astype(str).str.strip("'"),
    format="%Y-%m-%d %H:%M:%S.%f",
    errors="coerce",
).dt.date
```

### 3.2. Merge —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ñ–∏—á

–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ (`client_id`, `trans_date`):

```python
df = data.merge(
    data2,
    on=["client_id", "trans_date"],
    how="left",
)
```

---

## 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ —Ç–∏–ø–æ–≤

–ü–æ—Å–ª–µ merge –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å –ø—Ä–æ–ø—É—Å–∫–∏:

```python
df.isna().sum()
```

–°—Ç—Ä–∞—Ç–µ–≥–∏—è:

* –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è `is_fraud` ‚Äî –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤.

* `amount` –∏ —á–∏—Å–ª–æ–≤—ã–µ —Ñ–∏—á–∏ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –∫ `float`:

  ```python
  df["amount"] = df["amount"].astype(str).str.replace(",", ".").astype(float)
  ```

* –î–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ `data2`:

  * –ü—Ä–æ–ø—É—Å–∫–∏ **–Ω–µ —É–¥–∞–ª—è—é—Ç—Å—è**, —Ç.–∫. –∏—Ö –Ω–∞–ª–∏—á–∏–µ —É–∂–µ —Å–∞–º–æ –ø–æ —Å–µ–±–µ —Å–∏–≥–Ω–∞–ª.
  * –î–ª—è –∫–∞–∂–¥–æ–π —á–∏—Å–ª–æ–≤–æ–π —Ñ–∏—á–∏:

    * —Å–æ–∑–¥–∞–ª–∏ —Ñ–ª–∞–≥ `<feature>_was_missing` (1, –µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –±—ã–ª–æ NaN),
    * NaN –∑–∞–º–µ–Ω–∏–ª–∏ –Ω–∞ 0.

–ü—Ä–∏–º–µ—Ä:

```python
behavior_features = [
    "os_ver_cnt_30d",
    "phone_model_cnt_30d",
    "login_sessions_7d",
    "login_sessions_30d",
    "logins_per_day_7d",
    "logins_per_day_30d",
    "login_freq_change_7d_vs_30d",
    "logins_7d_share_of_30d",
    "avg_session_interval_30d",
    "std_session_interval_30d",
    "var_session_interval_30d",
    "ewm_session_interval_7d",
    "burstiness_sessions",
    "fano_factor_sessions",
    "zscore_interval_7d_vs_30d",
]

for col in behavior_features:
    if col not in df.columns:
        continue
    flag_col = f"{col}_was_missing"
    df[flag_col] = df[col].isna().astype(int)
    df[col] = pd.to_numeric(df[col], errors="coerce")
    df[col] = df[col].fillna(0.0)
```

**–í–∞–∂–Ω–æ:** –≤—Å–µ —Ñ–∏—á–∏ –≤ –∏—Ç–æ–≥–µ –∏–º–µ—é—Ç —Ç–∏–ø `float`, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç XGBoost.

---

## 5. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏

–ò–∑ –æ–±—â–µ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ–º X –∏ y:

```python
target_col = "is_fraud"

drop_cols = [
    target_col,
    "client_id",
    "transaction_id",
    "destination_id",
    "transdate",
    "transdatetime",
    "trans_date",
]

drop_cols = [c for c in drop_cols if c in df.columns]

X = df.drop(columns=drop_cols)
y = df[target_col].astype(int)
```

Train/validation split (random):

```python
from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(
    X,
    y,
    test_size=0.2,
    stratify=y,
    random_state=42,
)
```

–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:

```python
import numpy as np

neg, pos = np.bincount(y_train)
scale_pos_weight = neg / pos
print("scale_pos_weight:", scale_pos_weight)
# ‚âà 82.6
```

---

## 6. –û–±—É—á–µ–Ω–∏–µ XGBoost

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `XGBClassifier` —Å —É—á—ë—Ç–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞:

```python
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=400,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="auc",
    tree_method="hist",
    scale_pos_weight=scale_pos_weight,
    n_jobs=-1,
    random_state=42,
)

model.fit(
    X_train,
    y_train,
    eval_set=[(X_valid, y_valid)],
    verbose=50,
)
```

–û—Ü–µ–Ω–∫–∞:

```python
from sklearn.metrics import roc_auc_score, average_precision_score

y_proba_valid = model.predict_proba(X_valid)[:, 1]
roc = roc_auc_score(y_valid, y_proba_valid)
pr = average_precision_score(y_valid, y_proba_valid)

print("ROC-AUC:", roc)
print("PR-AUC: ", pr)
# ROC-AUC: ‚âà 0.8798
# PR-AUC:  ‚âà 0.4767
```

---

## 7. –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–æ–≤ —Ä–∏—Å–∫–∞

–¶–µ–ª—å ‚Äî –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ *–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å* –≤ —Ç—Ä–∏ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞:

* `low` ‚Äî –∞–≤—Ç–æ–æ–¥–æ–±—Ä–µ–Ω–∏–µ,
* `medium` ‚Äî soft-check (push/SMS),
* `high` ‚Äî –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ / —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞.

### 7.1. –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ –ø–æ –ø–æ—Ä–æ–≥–∞–º

```python
import numpy as np
import pandas as pd
from sklearn.metrics import precision_score, recall_score, confusion_matrix

thresholds = np.linspace(0.0, 1.0, 101)
rows = []

for thr in thresholds:
    y_pred = (y_proba_valid >= thr).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_valid, y_pred).ravel()

    precision = precision_score(y_valid, y_pred, zero_division=0)
    recall = recall_score(y_valid, y_pred, zero_division=0)
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0

    rows.append({
        "threshold": thr,
        "precision": precision,
        "recall": recall,
        "FPR": fpr,
        "TP": tp,
        "FP": fp,
        "FN": fn,
        "TN": tn,
    })

thr_df = pd.DataFrame(rows)
```

### 7.2. –í—ã–±—Ä–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏

–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ `thr_df` (precision/recall/FPR) –≤—ã–±—Ä–∞–Ω—ã:

* `p < 0.26` ‚Üí **low risk** ‚Äî –∞–≤—Ç–æ–æ–¥–æ–±—Ä–µ–Ω–∏–µ, –∫—Ä–∞–π–Ω–µ –Ω–∏–∑–∫–∏–π FPR.
* `0.26 ‚â§ p < 0.80` ‚Üí **medium risk** ‚Äî soft-–ø—Ä–æ–≤–µ—Ä–∫–∞ (push/SMS).
* `p ‚â• 0.80` ‚Üí **high risk** ‚Äî –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ / —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞.

–≠—Ç–∞ –ª–æ–≥–∏–∫–∞ –∑–∞—à–∏—Ç–∞ –≤ backend:

```python
def get_risk_level(proba: float) -> str:
    if proba >= 0.80:
        return "high"
    if proba >= 0.26:
        return "medium"
    return "low"
```

---

## 8. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature importance)

–î–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è feature importance –∏–∑ XGBoost:

```python
importances = model.feature_importances_

feat_importance = pd.DataFrame({
    "feature": X_train.columns,
    "importance": importances,
}).sort_values("importance", ascending=False)

top_10_importance = feat_importance.head(10)
```

–¢–æ–ø-10 —Ñ–∏—á –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –æ—Ç—á—ë—Ç–µ/–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å:

* –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—á–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥–∏–Ω–æ–≤, —Å—É–º–º–∞, –≤–∑—Ä—ã–≤–Ω–æ—Å—Ç—å, –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –∏ —Ç.–¥.).

---

## 9. –õ–æ–∫–∞–ª—å–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è (Explainability, z-score)

–î–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–æ–¥—Ö–æ–¥:

1. –°—á–∏—Ç–∞—é—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ –∏ std –ø–æ train –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ–∏—á–∏.

2. –î–ª—è –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å—á–∏—Ç–∞–µ—Ç—Å—è z-score:

   [
   z = \frac{x - \text{mean}}{\text{std}}
   ]

3. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç—Å—è |z-score| –∏ importance, —á—Ç–æ–±—ã –≤—ã–¥–µ–ª–∏—Ç—å ‚Äú–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ‚Äù —Ñ–∏—á–∏.

–ü—Ä–∏–º–µ—Ä:

```python
train_means = X_train.mean()
train_stds = X_train.std(ddof=0).replace(0, 1e-9)

feat_stats = pd.DataFrame({
    "feature": X_train.columns,
    "mean": train_means.values,
    "std": train_stds.values,
    "importance": model.feature_importances_,
}).set_index("feature")


def explain_transaction(x_row: pd.Series, feat_stats: pd.DataFrame,
                        z_threshold: float = 2.0, top_k: int = 10) -> pd.DataFrame:
    x_row = x_row.copy()
    df = feat_stats.copy()

    df["value"] = x_row[df.index]
    df["z_score"] = (df["value"] - df["mean"]) / df["std"]
    df["abs_z"] = df["z_score"].abs()
    df["is_outlier"] = df["abs_z"] >= z_threshold
    df["score"] = df["importance"] * df["abs_z"]

    return df.sort_values("score", ascending=False)[
        ["value", "mean", "std", "z_score", "importance", "is_outlier"]
    ].head(top_k)
```

–≠—Ç–∏–º –±–ª–æ–∫–æ–º –º–æ–∂–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ (debug/–æ—Ç—á—ë—Ç/–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è).

---

## 10. Time-based split (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏) ‚Äî –∫–æ–Ω—Ü–µ–ø—Ç

–î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω –∫–æ–¥:

1. –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ `transdatetime`.
2. –í–∑—è—Ç—å –ø–µ—Ä–≤—ã–µ 80% –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∫–∞–∫ train, –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20% ‚Äî –∫–∞–∫ test.
3. –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å ROC-AUC/PR-AUC —Å random split.

–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ:

```python
df_sorted = df.sort_values("transdatetime").reset_index(drop=True)

X_time = df_sorted.drop(columns=drop_cols)
y_time = df_sorted["is_fraud"].astype(int)

split_idx = int(len(df_sorted) * 0.8)
X_train_time = X_time.iloc[:split_idx]
y_train_time = y_time.iloc[:split_idx]
X_test_time = X_time.iloc[split_idx:]
y_test_time = y_time.iloc[split_idx:]

# –¥–∞–ª–µ–µ –æ–±—É—á–µ–Ω–∏–µ model_time –∏ –æ—Ü–µ–Ω–∫–∞ ROC-AUC/PR-AUC
```

–≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, **–Ω–µ ‚Äú–ø–æ–¥–≥–ª—è–¥—ã–≤–∞–µ—Ç‚Äù –ª–∏ –º–æ–¥–µ–ª—å –≤ –±—É–¥—É—â–µ–µ** –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å—Ç–∞–±–∏–ª—å–Ω–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏.

---

## 11. Baseline vs XGBoost ‚Äî —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (–∫–æ–¥)

–î–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω –∫–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç:

* `LogisticRegression` (—Å class_weight="balanced"),
* `RandomForestClassifier`,
* —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏—Ö —Å XGBoost –ø–æ:

  * ROC-AUC
  * PR-AUC
  * –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.

–ü—Ä–∏–º–µ—Ä:

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, average_precision_score
import time

results = []


def evaluate_model(name, clf, X_tr, y_tr, X_te, y_te):
    t0 = time.perf_counter()
    clf.fit(X_tr, y_tr)
    fit_time = time.perf_counter() - t0

    t1 = time.perf_counter()
    y_proba = clf.predict_proba(X_te)[:, 1]
    infer_time = time.perf_counter() - t1

    roc = roc_auc_score(y_te, y_proba)
    pr = average_precision_score(y_te, y_proba)
    time_per_1000 = infer_time * 1000 / len(X_te)

    results.append({
        "model": name,
        "ROC_AUC": roc,
        "PR_AUC": pr,
        "fit_time_sec": fit_time,
        "infer_time_per_1000_samples_ms": time_per_1000,
    })


log_reg = LogisticRegression(
    max_iter=1000,
    class_weight="balanced",
    n_jobs=-1,
    solver="saga",
)
evaluate_model("LogisticRegression", log_reg, X_train, y_train, X_valid, y_valid)

rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=7,
    class_weight="balanced_subsample",
    n_jobs=-1,
    random_state=42,
)
evaluate_model("RandomForest", rf, X_train, y_train, X_valid, y_valid)

# XGBoost ‚Äî —É–∂–µ –æ–±—É—á–µ–Ω
t0 = time.perf_counter()
y_proba_xgb = model.predict_proba(X_valid)[:, 1]
infer_time_xgb = time.perf_counter() - t0

roc_xgb = roc_auc_score(y_valid, y_proba_xgb)
pr_xgb = average_precision_score(y_valid, y_proba_xgb)
time_per_1000_xgb = infer_time_xgb * 1000 / len(X_valid)

results.append({
    "model": "XGBoost",
    "ROC_AUC": roc_xgb,
    "PR_AUC": pr_xgb,
    "fit_time_sec": None,
    "infer_time_per_1000_samples_ms": time_per_1000_xgb,
})

baseline_df = pd.DataFrame(results)
print(baseline_df)
```

–≠—Ç—É —Ç–∞–±–ª–∏—Ü—É –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é –∫–∞–∫:

> ‚Äú–ú—ã —Å—Ä–∞–≤–Ω–∏–ª–∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é, RandomForest –∏ XGBoost. XGBoost –¥–∞—ë—Ç –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ, –ø–æ—ç—Ç–æ–º—É –∏–º–µ–Ω–Ω–æ –æ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤ –ø—Ä–æ–¥–µ‚Äù.

(–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —á–∏—Å–ª–∞ –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ —ç—Ç–æ–≥–æ –±–ª–æ–∫–∞.)

---

## 12. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –¥–µ–ø–ª–æ–π –º–æ–¥–µ–ª–∏

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ `ml/models/model_xgb_baseline.pkl`:

```python
import joblib
joblib.dump(model, "ml/models/model_xgb_baseline.pkl")
```

–í backend –æ–Ω–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ `FraudModelService`:

```python
from backend.app.core.config import get_settings
import joblib

settings = get_settings()

class FraudModelService:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = joblib.load(self.model_path)
        self.feature_names = list(self.model.get_booster().feature_names)

    # –¥–∞–ª–µ–µ: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏—á, predict_proba, get_risk_level
```